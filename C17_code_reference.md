# C17.py - Справочное руководство по коду

## Общее назначение скрипта

Скрипт `C17.py` предназначен для автоматизированного сбора информации о тендерах с грузинского портала государственных закупок (tenders.procurement.gov.ge). Его основная цель — эффективно извлекать данные о тендерах по заданному CPV-коду и временному диапазону, сохранять HTML-содержимое страниц вкладок каждого тендера, а также накапливать все найденные ссылки на прикрепленные документы.

Ключевые особенности:
*   **Гибкий поиск:** Поддержка CPV-кодов, настраиваемых дат начала/окончания и диапазона страниц.
*   **"Умное" обновление:** Эффективная обработка уже существующих данных для минимизации повторных загрузок и парсинга, особенно при инкрементальных обновлениях.
*   **Накопительное сохранение ссылок:** Гарантированное сохранение всех когда-либо найденных ссылок на документы без дублирования.
*   **Модульность:** Использование `config.py` для управления путями и настройками.

## Ключевые технологии и зависимости

*   **Python 3:** Основной язык разработки.
*   **Selenium WebDriver:** Используется для автоматизации взаимодействия с веб-страницами в браузере Firefox (в headless режиме) через `geckodriver`.
*   **Pandas:** Для эффективной работы с табличными данными (CSV-файлы), включая чтение, запись и сравнение данных.
*   **BeautifulSoup4:** Для парсинга HTML-содержимого страниц и извлечения необходимых данных (App ID, Token, номер тендера, даты, статусы).
*   **`logging` модуль:** Для структурированного вывода информации о ходе выполнения скрипта, предупреждений и ошибок.
*   **`config.py`:** Внешний конфигурационный файл для управления путями к директориям, `geckodriver`, Firefox и базовым URL.

## Аргументы командной строки

Скрипт `C17.py` принимает следующие аргументы:

*   `-c`, `--cpv` (обязательный): CPV-код, по которому будет осуществляться поиск тендеров (например, `45200000`).
*   `-root`, `--root-dir` (опциональный): Корневая директория для сохранения всех данных. Если не указана, используется `DEFAULT_ROOT_DIR` из `config.py`.
*   `-ds`, `--date-start` (опциональный): Дата начала периода поиска в формате `DD.MM.YYYY`. По умолчанию — 1-е число предыдущего месяца.
*   `-de`, `--date-end` (опциональный): Дата окончания периода поиска в формате `DD.MM.YYYY`. По умолчанию — вчерашний день.
*   `-ps`, `--page-start` (опциональный): Начальная страница результатов поиска (по умолчанию `1`).
*   `-pe`, `--page-end` (опциональный): Конечная страница результатов поиска. Если `0`, обрабатываются все страницы до конца.
*   `--update` (флаг): Если активен, принудительно обновляет данные для всех найденных тендеров, даже если их метаданные не изменились. Полезно для полного "рефреша".

## Алгоритм работы скрипта

### 0. Инициализация и настройка

1.  **Парсинг аргументов:** Обработка всех аргументов командной строки. Установка дат по умолчанию при необходимости.
2.  **Настройка логирования:** Инициализация стандартного Python-модуля `logging` для вывода информации, предупреждений и ошибок. Уровень логирования установлен на `INFO`.
3.  **Получение путей:** Через `config.py` определяются и при необходимости создаются все необходимые директории для хранения данных (HTML-файлов, CSV-файлов, загруженных документов).
4.  **Настройка WebDriver:** Инициализация Selenium WebDriver с использованием Firefox в headless режиме (`options.add_argument("--headless")`) и `geckodriver`.

### 1. STAGE 1: Сбор общей информации о тендерах со страниц поиска

1.  **Загрузка существующих метаданных:** Скрипт считывает `data_urls.csv` (если он существует) и загружает в `pandas.DataFrame` всю информацию о ранее собранных тендерах: `application_id`, `tender_num`, `tender_start`, `tender_end`, `tender_status`. Эти данные будут использоваться для "умного" обновления.
2.  **Выполнение поиска:** WebDriver переходит на страницу портала госзакупок, выбирает указанный CPV-код, вводит даты начала и окончания (используя `driver.execute_script` из-за особенностей полей ввода) и нажимает кнопку "Поиск".
3.  **Итерация по страницам результатов:** Скрипт переходит по страницам результатов поиска (от `START_PAGE` до `END_PAGE` или до последней страницы).
4.  **Извлечение базовых данных тендера:** Для каждой страницы с помощью `BeautifulSoup4` извлекаются `App ID`, `Token`, номер тендера, даты начала/окончания и текущий статус тендера. Эти данные временно сохраняются в `all_data`.

### 2. STAGE 2: Детальный сбор данных по вкладкам и "умное" обновление

Этот этап является ключевым для реализации логики "умного" обновления.

1.  **Итерация по собранным тендерам:** Скрипт поочередно обрабатывает каждый тендер из `all_data`.
2.  **Принятие решения об обновлении:**
    *   Если флаг `--update` **активен**, тендер принудительно помечается для обработки.
    *   Если `--update` **НЕ активен**:
        *   Скрипт ищет текущий `App ID` в загруженном `existing_tenders_df`.
        *   Если тендер **новый** (не найден в `existing_tenders_df`), он помечается для обработки.
        *   Если тендер **существует**, его текущие метаданные (`tender_num`, `tender_start`, `tender_end`, `tender_status`) сравниваются с сохраненными.
        *   Если **хоть одно метаданное изменилось**, тендер помечается для обработки.
        *   Если тендер существует и **метаданные не изменились**, тендер пропускается (его вкладки не будут загружаться, HTML не будет перезаписываться, ссылки не будут повторно извлекаться).
3.  **Обработка тендера (если требуется обновление):**
    *   **Переход по вкладкам:** Скрипт последовательно переходит по всем детальным вкладкам тендера (`first_tab`, `app_main`, `app_docs`, `app_bids`, `agency_docs`, `agr_docs`).
    *   **Сохранение HTML:** HTML-содержимое каждой вкладки **перезаписывается** в соответствующий файл в директории `html_tabs`. Это гарантирует, что мы всегда имеем актуальную версию страницы для поиска новых ссылок.
    *   **Сбор ссылок (накопительный):**
        *   Из только что сохраненного HTML каждой вкладки извлекаются все ссылки.
        *   Эти ссылки сравниваются с уже существующими ссылками для данного `tender_code` и `App ID` в `LINKS_CSV_FILE` (если он существует).
        *   **Только новые, уникальные ссылки** (которых ранее не было в `LINKS_CSV_FILE` для этого тендера) добавляются во временный глобальный список `all_links`. Важно: старые ссылки не удаляются.

### 3. Финальное сохранение данных

1.  **Сохранение `LINKS_CSV_FILE`:**
    *   Если существуют новые ссылки в `all_links`, они преобразуются в `DataFrame`.
    *   Скрипт сравнивает эти новые ссылки с уже существующими в `LINKS_CSV_FILE` (если файл есть), используя комбинацию `url`, `tender`, `tab_name`, `text` для создания уникального идентификатора.
    *   **Только действительно новые и уникальные ссылки** добавляются в `LINKS_CSV_FILE` в режиме `append`. Если файла не было, он создается. Это обеспечивает монотонно растущий список всех найденных файлов.
2.  **Сохранение `CSV_FILE` (`data_urls.csv`):**
    *   Все собранные и обновленные метаданные тендеров (`all_data`) преобразуются в `DataFrame`.
    *   Этот `DataFrame` объединяется с `existing_tenders_df`. Дубликаты по `application_id` удаляются, сохраняя *последнюю* (самую актуальную) версию записи тендера.
    *   Результат сохраняется в `CSV_FILE`, обеспечивая актуальную базу данных по статусам и датам тендеров.

### 4. Завершение работы

1.  WebDriver завершает работу (`driver.quit()`).
2.  Выводится итоговое сообщение о статусе завершения скрипта.

## Обоснование принятых решений

*   **Внедрение `logging`:** Переход от `print()` к `logging` был сделан для повышения гибкости и контроля над выводом информации. Это критично для масштабируемых проектов, позволяя легко управлять уровнями детализации логов (INFO, WARNING, ERROR), направлять их в файлы и упрощать отладку.
*   **Логика "умного" обновления:** Необходима для эффективной работы с большим объемом данных (30,000 документов) и регулярных обновлений (еженедельно). Полная перезапись всего каждый раз была бы слишком ресурсоемкой. Сравнение метаданных позволяет значительно сократить время и трафик, обрабатывая только изменившиеся или новые записи.
*   **Накопительное сохранение ссылок:** Требование "прикрепленные файлы не будут изменяться, могут только добавиться новые" привело к реализации логики, которая гарантирует, что `LINKS_CSV_FILE` является полным историческим списком всех ссылок. Даже если ссылка исчезнет со страницы, она останется в нашем локальном архиве.
*   **`href.replace("library/library", "library")`:** Это решение оставлено как есть, поскольку предполагается, что это обход специфического бага на целевом веб-сайте. Несмотря на то, что это "костыль", он выполняет свою функцию, и без дальнейшего исследования причины на стороне сайта, это наиболее прагматичный подход.
*   **`driver.execute_script` для установки дат:** Использование `execute_script` вместо `send_keys` для полей дат выбрано из-за того, что `send_keys` не срабатывал. Это обеспечивает надежную установку значений полей, обходя возможные JavaScript-валидации или блокировки прямого ввода.

## Инструкции по запуску

Для запуска скрипта используйте bash-скрипт `run_01` (или аналогичный), который активирует ваше виртуальное окружение и вызывает `C17.py` с необходимыми аргументами.

Пример `run_01` :

```bash
#!/bin/bash
source ~/projects/venv/bin/activate
python C17.py -root ML_DATA/ -c 45200000 -ds 01.01.2024 -de 10.01.2024 -ps 1 -pe 5
deactivate
```

Убедитесь, что `geckodriver` и `firefox` доступны по путям, указанным в `config.py`, и что все Python-зависимости (pandas, selenium, beautifulsoup4) установлены в вашем виртуальном окружении.
