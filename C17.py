#!/usr/bin/env python3
from urllib.parse import urlencode
from selenium import webdriver
from selenium.webdriver.support.ui import Select, WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
import pandas as pd
import time
import logging

import os
import argparse
import re
import config
from datetime import datetime, timedelta

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ---
def get_default_dates():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é:
    - date_start: 1 –¥–µ–Ω—å –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –º–µ—Å—è—Ü–∞
    - date_end: –≤—á–µ—Ä–∞—à–Ω–∏–π –¥–µ–Ω—å
    –§–æ—Ä–º–∞—Ç: "DD.MM.YYYY"
    """
    today = datetime.now()
    
    # –í—á–µ—Ä–∞—à–Ω–∏–π –¥–µ–Ω—å
    date_end = (today - timedelta(days=1)).strftime("%d.%m.%Y")
    
    # 1 –¥–µ–Ω—å –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –º–µ—Å—è—Ü–∞
    first_day_prev_month = today.replace(day=1) - timedelta(days=1)
    first_day_prev_month = first_day_prev_month.replace(day=1)
    date_start = first_day_prev_month.strftime("%d.%m.%Y")
    
    return date_start, date_end

# --- –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ ---
def parse_arguments():
    parser = argparse.ArgumentParser(description='–ü–∞—Ä—Å–µ—Ä —Ç–µ–Ω–¥–µ—Ä–æ–≤ —Å –ø–æ—Ä—Ç–∞–ª–∞ –≥–æ—Å–∑–∞–∫—É–ø–æ–∫')
    parser.add_argument('-c', '--cpv', type=str, required=True, help='CPV –∫–æ–¥ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π)')
    parser.add_argument('-root', '--root-dir', type=str, help='–ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    parser.add_argument('-ds', '--date-start', type=str, help='–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ (—Ñ–æ—Ä–º–∞—Ç: DD.MM.YYYY)')
    parser.add_argument('-de', '--date-end', type=str, help='–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è (—Ñ–æ—Ä–º–∞—Ç: DD.MM.YYYY)')
    parser.add_argument('-ps', '--page-start', type=int, help='–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞')
    parser.add_argument('-pe', '--page-end', type=int, help='–ö–æ–Ω–µ—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞')
    parser.add_argument('--update', action='store_true', help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç.')
    
    args = parser.parse_args()
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –¥–∞—Ç
    DATE_START, DATE_END = get_default_dates()
    DATE_FROM = args.date_start if args.date_start else DATE_START
    DATE_TILL = args.date_end if args.date_end else DATE_END
    
    # Handle page arguments
    START_PAGE = args.page_start if args.page_start is not None else 1
    
    # If -pe is 0, it means process all pages until the end
    if args.page_end == 0:
        PAGE_END_ARG = None
    else:
        PAGE_END_ARG = args.page_end
    
    return args.cpv, DATE_FROM, DATE_TILL, START_PAGE, PAGE_END_ARG, args.root_dir, args.update

# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü ---
def extract_total_pages(pagination_info):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ pagination_info
    –ü—Ä–∏–º–µ—Ä: '52 ·É©·Éê·Éú·Éê·É¨·Éî·É†·Éò (·Éí·Éï·Éî·É†·Éì·Éò: 1/13)' ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 13
    """
    try:
        match = re.search(r'·Éí·Éï·Éî·É†·Éì·Éò:\s*(\d+)/(\d+)', pagination_info)
        if match:
            return int(match.group(2))
    except:
        pass
    return 1

# --- –ü–æ–ª—É—á–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã ---
TARGET_CPV_CODE, DATE_FROM, DATE_TILL, START_PAGE, PAGE_END_ARG, ROOT_DIR_ARG, UPDATE_FLAG = parse_arguments()



# --- –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ CPV –∫–æ–¥–∞ ---
PATHS = config.get_project_paths(TARGET_CPV_CODE)

# –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –ø—É—Ç–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
BASE_DIR = PATHS['BASE_DIR']
CSV_FILE = PATHS['CSV_FILE']
LINKS_CSV_FILE = PATHS['LINKS_CSV_FILE']
OUTPUT_DIR = PATHS['HTML_DIR']
GECKODRIVER_PATH = config.GECKODRIVER_PATH
FIREFOX_PATH = config.FIREFOX_PATH

# --- WebDriver setup ---
options = Options()
options.add_argument("--headless") # Enabled --headless option
options.binary_location = FIREFOX_PATH
service = Service(executable_path=GECKODRIVER_PATH)
driver = webdriver.Firefox(service=service, options=options)
wait = WebDriverWait(driver, 20)

# --- –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤–∫–ª–∞–¥–æ–∫ ---
def build_tab_urls(app_id, token):
    base_url = "https://tenders.procurement.gov.ge/public/library/controller.php"
    return {
        "first_tab": f"{base_url}?action=application&app_id={app_id}&app_reg=&key={token}",
        "app_main": f"{base_url}?action=app_main&app_id={app_id}&key={token}",
        "app_docs": f"{base_url}?action=app_docs&app_id={app_id}&key={token}",
        "app_bids": f"{base_url}?action=app_bids&app_id={app_id}&key={token}",
        "agency_docs": f"{base_url}?action=agency_docs&app_id={app_id}&key={token}",
        "agr_docs": f"{base_url}?action=agr_docs&app_id={app_id}"  # –±–µ–∑ token
    }

def save_tab_pages(driver, app_id, token, page_num, all_links_global, tender_no, tender_code, tdr_start, tdr_end, tdr_status):
    """
    –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç –ø–æ –≤—Å–µ–º –≤–∫–ª–∞–¥–∫–∞–º App ID, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ.
    –î–æ–±–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫.
    """
    tabs = build_tab_urls(app_id, token)
    current_tender_new_links = [] # –°—Å—ã–ª–∫–∏, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ —Ç–µ–∫—É—â–µ–º —Ç–µ–Ω–¥–µ—Ä–µ

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ tender_code, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    existing_links_for_cpv = pd.DataFrame()
    if os.path.exists(LINKS_CSV_FILE):
        try:
            existing_links_for_cpv = pd.read_csv(LINKS_CSV_FILE, dtype={'tender_code': str, 'tender': str})
            existing_links_for_cpv = existing_links_for_cpv[existing_links_for_cpv['tender_code'] == tender_code]
            existing_links_for_cpv = existing_links_for_cpv[existing_links_for_cpv['tender'] == f"TENDER_{app_id}"]
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {LINKS_CSV_FILE} –¥–ª—è App ID {app_id}: {e}")


    for tab_name, tab_url in tabs.items():
        try:
            driver.get(tab_url)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            time.sleep(1)

            # --- Save HTML ---
            filename = os.path.join(OUTPUT_DIR, f"pg_{tender_no}_{app_id}_{tab_name}.html")
            with open(filename, "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            logger.info(f"üíæ Page {page_num}: Saved {filename}")

            # --- Collect links ---
            links = driver.find_elements(By.TAG_NAME, "a")
            for a in links:
                text = a.text.strip()
                href = a.get_attribute("href")
                if href:
                    href = href.replace("library/library", "library")
                    link_data = {
                        "tender_code": tender_code,
                        "tender_name": tender_no,
                        "tender": f"TENDER_{app_id}",
                        "tab_name": tab_name,
                        "text": text,
                        "url": href,
                        "tender_start": tdr_start,
                        "tender_end": tdr_end,
                        "tender_status": tdr_status
                    }
                    current_tender_new_links.append(link_data)

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Page {page_num}: Failed to save tab '{tab_name}': {e}")
            continue

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ all_links_global
    if current_tender_new_links:
        current_tender_new_links_df = pd.DataFrame(current_tender_new_links)

        if not existing_links_for_cpv.empty:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏ –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏, —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞–≤–ª—è—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –º—ã –Ω–µ –±—É–¥–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Å—ã–ª–∫–∏
            combined_df = pd.concat([existing_links_for_cpv, current_tender_new_links_df])
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ 'url', 'tender', 'tab_name', 'text'
            # 'keep='first'' –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ (–∏–∑ existing_links_for_cpv) –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã, –∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ new_links_df —É–¥–∞–ª–µ–Ω—ã
            unique_new_links_df = combined_df.drop_duplicates(subset=['url', 'tender', 'tab_name', 'text'], keep=False)
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–∏ –≤ existing_links_for_cpv (—Ç.–µ. –Ω–æ–≤—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ)
            # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–∂–Ω–æ–π –ª–æ–≥–∏–∫–æ–π, –ø—Ä–æ—â–µ –¥–æ–±–∞–≤–∏—Ç—å –≤—Å–µ –Ω–æ–≤—ã–µ –∏ –∑–∞—Ç–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å

            # –ë–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ, –∞ –∑–∞—Ç–µ–º –æ—Å—Ç–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ, –Ω–æ —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ —Ä–∞–Ω—å—à–µ
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            existing_links_for_cpv['link_id'] = existing_links_for_cpv['url'] + existing_links_for_cpv['tender'] + existing_links_for_cpv['tab_name'] + existing_links_for_cpv['text']
            current_tender_new_links_df['link_id'] = current_tender_new_links_df['url'] + current_tender_new_links_df['tender'] + current_tender_new_links_df['tab_name'] + current_tender_new_links_df['text']

            new_links_to_add_df = current_tender_new_links_df[~current_tender_new_links_df['link_id'].isin(existing_links_for_cpv['link_id'])]
            new_links_to_add_df = new_links_to_add_df.drop(columns=['link_id'])
            
            if not new_links_to_add_df.empty:
                all_links_global.extend(new_links_to_add_df.to_dict(orient='records'))
                logger.info(f"   -> App ID {app_id}: –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_links_to_add_df)} –Ω–æ–≤—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫.")
            else:
                logger.info(f"   -> App ID {app_id}: –ù–æ–≤—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

        else:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ LINKS_CSV_FILE –Ω–µ –±—ã–ª–æ, –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ - –Ω–æ–≤—ã–µ
            all_links_global.extend(current_tender_new_links)
            logger.info(f"   -> App ID {app_id}: –î–æ–±–∞–≤–ª–µ–Ω–æ {len(current_tender_new_links)} —Å—Å—ã–ª–æ–∫ (—Ñ–∞–π–ª LINKS_CSV_FILE —Å–æ–∑–¥–∞–Ω –≤–ø–µ—Ä–≤—ã–µ).")

def perform_search():
    """
    –û—Ç–∫—Ä—ã–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞, –≤—ã–±–∏—Ä–∞–µ—Ç CPV –∫–æ–¥, –∑–∞–¥–∞–µ—Ç –¥–∞—Ç—ã –∏ –Ω–∞–∂–∏–º–∞–µ—Ç Search.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –∏ —Ç–∞–±–ª–∏—Ü–∞ —Ç–µ–Ω–¥–µ—Ä–æ–≤ –¥–æ—Å—Ç—É–ø–Ω–∞.
    """
    driver.get("https://tenders.procurement.gov.ge/public/?lang=ge")

    # --- Select CPV code ---
    select_elem = wait.until(EC.presence_of_element_located((By.ID, "app_basecode")))
    select = Select(select_elem)
    for option in select.options:
        if TARGET_CPV_CODE in option.text:
            select.select_by_visible_text(option.text)
            break

    # --- Fill date fields ---
    driver.execute_script(f"document.getElementById('app_date_from').value='{DATE_FROM}'")
    driver.execute_script(f"document.getElementById('app_date_till').value='{DATE_TILL}'")

    # --- Click Search ---
    wait.until(EC.element_to_be_clickable((By.ID, "search_btn"))).click()
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#content tbody tr")))
    time.sleep(1)


def load_existing_app_ids(csv_file_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ App ID –∏ –∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ CSV —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å App ID, –Ω–æ–º–µ—Ä–æ–º —Ç–µ–Ω–¥–µ—Ä–∞, –¥–∞—Ç–∞–º–∏ –∏ —Å—Ç–∞—Ç—É—Å–æ–º.
    """
    existing_df = pd.DataFrame(columns=['application_id', 'tender_num', 'tender_start', 'tender_end', 'tender_status'])
    if os.path.exists(csv_file_path):
        try:
            df = pd.read_csv(csv_file_path, dtype={'application_id': str})
            required_cols = ['application_id', 'tender_num', 'tender_start', 'tender_end', 'tender_status']
            if all(col in df.columns for col in required_cols):
                existing_df = df[required_cols]
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(existing_df)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π —Ç–µ–Ω–¥–µ—Ä–æ–≤ –∏–∑ {csv_file_path}")
            else:
                logger.warning(f"‚ö†Ô∏è CSV —Ñ–∞–π–ª {csv_file_path} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∫–∞–∫ –Ω–æ–≤—ã–π.")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {csv_file_path}: {e}")
    return existing_df

from bs4 import BeautifulSoup

def parse_urls(html):
    """
    Extract App IDs and Tokens from the page without going inside tenders.
    Returns:
        page_data: list of dicts with App ID and Token
        pagination_info: text info about page
    """
    soup = BeautifulSoup(html, "html.parser")
    rows = soup.select("#content tbody tr")

    announcement_tags = soup.select("p:-soup-contains('·Éí·Éê·Éú·É™·ÉÆ·Éê·Éì·Éî·Éë·Éò·É° ·Éú·Éù·Éõ·Éî·É†·Éò:') > strong")
    tenders_n = [tag.get_text().strip() for tag in announcement_tags]

    # dates
    start_date_tags = soup.select("p:-soup-contains('·É®·Éî·É°·Éß·Éò·Éì·Éï·Éò·É° ·Éí·Éê·Éõ·Éù·É™·ÉÆ·Éê·Éì·Éî·Éë·Éò·É° ·Éó·Éê·É†·Éò·É¶·Éò:')")
    tg_starts = [tag.get_text().split(':')[-1].strip() for tag in start_date_tags]

    # Select all p tags that contain the phrase "·É¨·Éò·Éú·Éì·Éê·Éì·Éî·Éë·Éî·Éë·Éò·É° ·Éõ·Éò·É¶·Éî·Éë·Éò·É° ·Éï·Éê·Éì·Éê:"
    # Then get the text from those tags.
    end_date_tags = soup.select("p:-soup-contains('·É¨·Éò·Éú·Éì·Éê·Éì·Éî·Éë·Éî·Éë·Éò·É° ·Éõ·Éò·É¶·Éî·Éë·Éò·É° ·Éï·Éê·Éì·Éê:')")
    tg_ends = [tag.get_text().split(':')[-1].strip() for tag in end_date_tags]

    # ststius
    status_tags = soup.select('p.status')

    # –°–æ–∑–¥–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    all_statuses = [tag.get_text(strip=True) for tag in status_tags]

    # tenders_n = soup.select("#content tbody ·Éí·Éê·Éú·É™·ÉÆ·Éê·Éì·Éî·Éë·Éò·É° ·Éú·Éù·Éõ·Éî·É†·Éò: strong")
    # breakpoint()
    page_data = []

    for row in rows:
        onclick = row.get("onclick", "")
        app_id, token = "", ""
        m = re.search(r"ShowApp\((\d+),\s*'[^']*',\s*\d+,\s*'([^']+)'\)", onclick)
        if m:
            app_id, token = m.groups()

        if not app_id or not token:
            continue

        page_data.append({
            "App ID": app_id,
            "Token": token
        })

    # Pagination info
    pagination_span = soup.find("span", string=lambda s: s and "·É©·Éê·Éú·Éê·É¨·Éî·É†·Éò" in s)
    pagination_info = pagination_span.get_text(strip=True) if pagination_span else "Not found"

    return page_data, pagination_info, tenders_n, tg_starts, tg_ends, all_statuses

# --- Main ---
all_data = []
all_links = []

try:
    logger.info(f"üéØ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞:")
    logger.info(f"   CPV –∫–æ–¥: {TARGET_CPV_CODE}")
    logger.info(f"   –ü–µ—Ä–∏–æ–¥: {DATE_FROM} - {DATE_TILL}")
    logger.info(f"   –ü—Ä–æ–µ–∫—Ç: {BASE_DIR}")
    
    # --- –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ App ID –∏ –∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ ---
    existing_tenders_df = load_existing_app_ids(CSV_FILE)
    existing_app_ids_set = set(existing_tenders_df['application_id'].astype(str)) if not existing_tenders_df.empty else set()
    
    # --- STAGE 1: Collect all tender information from all search pages ---
    logger.info("\n--- STAGE 1: Collecting tender information from all pages ---")
    
    perform_search()
    
    page_html = driver.page_source
    _, pagination_info, _, _, _, _ = parse_urls(page_html)
    total_pages = extract_total_pages(pagination_info)
    END_PAGE = PAGE_END_ARG if PAGE_END_ARG is not None else total_pages
    if END_PAGE > total_pages:
        logger.warning(f"‚ö†Ô∏è –ó–∞–ø—Ä–æ—à–µ–Ω–Ω–∞—è –∫–æ–Ω–µ—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {END_PAGE} –±–æ–ª—å—à–µ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü {total_pages}")
        END_PAGE = total_pages
    
    logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ: {pagination_info}")
    logger.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {START_PAGE}-{END_PAGE} –∏–∑ {total_pages}")

    # This single loop handles both pagination and conditional scraping
    for current_page in range(1, END_PAGE + 1):
        
        # --- Pagination Logic: If we're past page 1, click to the next page ---
        if current_page > 1:
            logger.info(f"   -> Navigating to page {current_page}...")
            try:
                next_btn = wait.until(EC.presence_of_element_located((By.ID, "btn_next")))
                driver.execute_script("arguments[0].click();", next_btn)
                wait.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR, "div.pager button.ui-button-text-icon-primary"), f"·Éí·Éï·Éî·É†·Éì·Éò: {current_page}/"))
                time.sleep(1)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Page {current_page}: Failed to navigate. Error: {e}. Stopping pagination.")
                break
        
        # --- Scraping Logic: Only scrape if the page is in the desired range ---
        if current_page >= START_PAGE:
            logger.info(f"\n--- Processing page {current_page} ---")
            page_html = driver.page_source
            page_data, _, tenders, tenders_start, tenders_end, all_tender_statuses = parse_urls(page_html)
            
            if not page_data:
                 logger.warning(f"‚ö†Ô∏è Page {current_page}: No tenders found on this page. Continuing...")
                 continue

            logger.info(f"‚úÖ Page {current_page}: Found {len(page_data)} tenders")

            for item, tender_num, tender_start, tender_end, tender_status in zip(page_data, tenders, tenders_start, tenders_end, all_tender_statuses):
                all_data.append({
                    "App ID": item["App ID"],
                    "Token": item["Token"],
                    "Tender Num": tender_num,
                    "Tender Start": tender_start,
                    "Tender end": tender_end,
                    "Tender Status": tender_status
                })


    logger.info(f"\n--- STAGE 1 COMPLETE: Collected {len(all_data)} total tenders ---")

    # --- STAGE 2: Scrape all the detail tabs for each collected tender ---
    logger.info("\n--- STAGE 2: Scraping detail tabs for each tender ---")
    if UPDATE_FLAG:
        logger.info("üí° –§–ª–∞–≥ --update –∞–∫—Ç–∏–≤–µ–Ω: –±—É–¥—É—Ç –æ–±–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–µ–Ω–¥–µ—Ä—ã.")

    processed_count = 0
    skipped_count = 0
    for index, tender_info in enumerate(all_data):
        app_id = tender_info["App ID"]
        token = tender_info["Token"]
        tender_num = tender_info["Tender Num"]
        tender_start = tender_info["Tender Start"]
        tender_end = tender_info["Tender end"]
        tender_status = tender_info["Tender Status"]

        should_process = True
        skip_reason = ""

        if not UPDATE_FLAG:
            # Check if tender already exists in the DataFrame and if metadata has changed
            existing_tender = existing_tenders_df[existing_tenders_df['application_id'] == str(app_id)]

            if not existing_tender.empty:
                existing_tender = existing_tender.iloc[0]
                if (
                    existing_tender['tender_num'] == tender_num and
                    existing_tender['tender_start'] == tender_start and
                    existing_tender['tender_end'] == tender_end and
                    existing_tender['tender_status'] == tender_status
                ):
                    should_process = False
                    skip_reason = "—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å"
                else:
                    # Metadata changed, so we need to re-process and update
                    skip_reason = "—Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ)"
            else:
                # New tender, not in existing_tenders_df
                skip_reason = "–Ω–æ–≤—ã–π —Ç–µ–Ω–¥–µ—Ä"
        else:
            skip_reason = "—Ñ–ª–∞–≥ --update –∞–∫—Ç–∏–≤–µ–Ω"

        if not should_process and not UPDATE_FLAG:
            logger.info(f"   -> –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–Ω–¥–µ—Ä {tender_num} (App ID: {app_id}): {skip_reason}.")
            skipped_count += 1
            continue
        
        logger.info(f"   -> –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–Ω–¥–µ—Ä–∞ {processed_count + skipped_count + 1}/{len(all_data)}: {tender_num} (App ID: {app_id}) - {skip_reason}")
        save_tab_pages(driver, app_id, token, (index + 1), all_links, tender_num, TARGET_CPV_CODE, tender_start, tender_end, tender_status)
        processed_count += 1

    logger.info("\n--- STAGE 2 COMPLETE ---")
    logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤—ã—Ö/–æ–±–Ω–æ–≤–ª–µ–Ω–æ —Ç–µ–Ω–¥–µ—Ä–æ–≤: {processed_count}")
    logger.info(f"‚ÑπÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–Ω–¥–µ—Ä–æ–≤: {skipped_count}")

    # --- Final Save ---
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ (LINKS_CSV_FILE) - –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞
    if all_links:
        current_links_df = pd.DataFrame(all_links)
        if os.path.exists(LINKS_CSV_FILE):
            existing_links_df = pd.read_csv(LINKS_CSV_FILE, dtype={'tender_code': str, 'tender': str, 'url': str, 'tab_name': str, 'text': str})
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º, —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ —Ä–∞–Ω–µ–µ
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            existing_links_df['link_id'] = existing_links_df['url'] + existing_links_df['tender'] + existing_links_df['tab_name'] + existing_links_df['text']
            current_links_df['link_id'] = current_links_df['url'] + current_links_df['tender'] + current_links_df['tab_name'] + current_links_df['text']

            new_links_to_save_df = current_links_df[~current_links_df['link_id'].isin(existing_links_df['link_id'])]
            new_links_to_save_df = new_links_to_save_df.drop(columns=['link_id'])
            
            if not new_links_to_save_df.empty:
                new_links_to_save_df.to_csv(LINKS_CSV_FILE, mode='a', header=False, index=False, encoding="utf-8")
                logger.info(f"üíæ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_links_to_save_df)} –Ω–æ–≤—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –≤ {LINKS_CSV_FILE}")
            else:
                logger.info(f"üíæ –ù–æ–≤—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ {LINKS_CSV_FILE}")
        else:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ –±—ã–ª–æ, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ
            current_links_df.to_csv(LINKS_CSV_FILE, index=False, encoding="utf-8")
            logger.info(f"üíæ –°–æ–∑–¥–∞–Ω {LINKS_CSV_FILE} —Å {len(current_links_df)} —Å—Å—ã–ª–∫–∞–º–∏.")
    else:
        logger.info(f"üíæ –ù–µ—Ç —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {LINKS_CSV_FILE}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ–Ω–¥–µ—Ä–æ–≤ (CSV_FILE) - –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ/–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ
    if all_data:
        new_tender_data_df = pd.DataFrame(all_data)
        new_tender_data_df = new_tender_data_df.drop(columns=['Tender Status'])
        new_tender_data_df = new_tender_data_df.rename(columns={"App ID": "application_id", "Token": "token", "Tender Num": "tender_num", "Tender Start": "tender_start", "Tender end": "tender_end"})
        new_tender_data_df = new_tender_data_df[["tender_num", "application_id", "token", "tender_start", "tender_end"]]
        
        if not existing_tenders_df.empty:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ
            # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ–Ω–¥–µ—Ä–æ–≤
            updated_tenders_df = pd.concat([existing_tenders_df, new_tender_data_df])
            updated_tenders_df = updated_tenders_df.drop_duplicates(subset=['application_id'], keep='last')
            updated_tenders_df.to_csv(CSV_FILE, index=False, encoding="utf-8")
            logger.info(f"üíæ –û–±–Ω–æ–≤–ª–µ–Ω–æ/–¥–æ–±–∞–≤–ª–µ–Ω–æ {len(new_tender_data_df)} –∑–∞–ø–∏—Å–µ–π —Ç–µ–Ω–¥–µ—Ä–æ–≤ –≤ {CSV_FILE}. –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(updated_tenders_df)}")
        else:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ –±—ã–ª–æ, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –Ω–æ–≤—ã–µ
            new_tender_data_df.to_csv(CSV_FILE, index=False, encoding="utf-8")
            logger.info(f"üíæ –°–æ–∑–¥–∞–Ω {CSV_FILE} —Å {len(new_tender_data_df)} –∑–∞–ø–∏—Å—è–º–∏ —Ç–µ–Ω–¥–µ—Ä–æ–≤.")
    else:
        # –ï—Å–ª–∏ all_data –ø—É—Å—Ç, –Ω–æ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–ª, –º—ã –µ–≥–æ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º.
        # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ –±—ã–ª–æ, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –∏–ª–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å.
        if not os.path.exists(CSV_FILE):
             pd.DataFrame(columns=["tender_num", "application_id", "token", "tender_start", "tender_end"]).to_csv(CSV_FILE, index=False, encoding="utf-8")
             logger.info(f"üíæ –°–æ–∑–¥–∞–Ω –ø—É—Å—Ç–æ–π {CSV_FILE}.")
        else:
             logger.info(f"üíæ –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ç–µ–Ω–¥–µ—Ä–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {CSV_FILE}")

    logger.info(f"‚úÖ Total tenders in {CSV_FILE}: {len(updated_tenders_df) if 'updated_tenders_df' in locals() else len(new_tender_data_df) if 'new_tender_data_df' in locals() else 0}")


except Exception as e:
    logger.error(f"‚ùå Error: {e}")

finally:
    driver.quit()

logger.info(f"‚úÖ {TARGET_CPV_CODE} OK - Clary_0")

